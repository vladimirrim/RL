{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSW6zQJeuLAl"
   },
   "outputs": [],
   "source": [
    "from gym import make\n",
    "\n",
    "env = make(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iQL1NyKRvP8V"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Normal\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden=400, in_dim=3, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc2 = nn.Linear(in_dim, hidden)\n",
    "        self.valNet = nn.Linear(hidden, 1)\n",
    "        ortho_init(self.fc2, weight_scale=1.0, constant_bias=0.0)\n",
    "        ortho_init(self.valNet, weight_scale=1.0, constant_bias=0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.fc2(x))\n",
    "        val = self.valNet(z)\n",
    "        return val\n",
    "\n",
    "def orthoInit(model):\n",
    "    ortho_init(model.fc, weight_scale=0.01, constant_bias=0.0)\n",
    "    ortho_init(model.sigma, weight_scale=0.01, constant_bias=0.0)\n",
    "    ortho_init(model.mu, weight_scale=0.01, constant_bias=0.0)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, hidden=400, in_dim=3, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, hidden)\n",
    "        self.mu = nn.Linear(hidden, out_dim)\n",
    "        self.sigma = nn.Parameter(torch.full((1,), np.log(0.6)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z2 = F.relu(self.fc(x))\n",
    "        mu = 2 * F.tanh(self.mu(z2))\n",
    "        sigma = self.sigma.expand_as(mu).exp()\n",
    "        return mu, sigma\n",
    "    \n",
    "    def save(self):\n",
    "            with open('agent.pkl', 'wb') as f:\n",
    "                pickle.dump(self, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "kXFMyhKUxFsc",
    "outputId": "21b0f1ba-234a-46a0-9510-e4cfec91765d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNIULrCzyUf-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def policy_loss(logProb, A):\n",
    "    polLoss = -(logProb * A).mean()\n",
    "    return polLoss\n",
    "\n",
    "def zeroGrads(ann):\n",
    "    ind = 0\n",
    "    for e in ann.parameters():\n",
    "        if e.grad is None:\n",
    "            continue\n",
    "        shape = e.size()\n",
    "        nParams = np.prod(shape)\n",
    "        e.grad.data *= 0\n",
    "        ind += nParams\n",
    "\n",
    "def value_loss(val, ret):\n",
    "    return ((val - ret) ** 2 / 2).mean()\n",
    "\n",
    "\n",
    "def getGrads(ann):\n",
    "    ret = []\n",
    "    for param, e in ann.named_parameters():\n",
    "          ret += e.grad.data.view(-1).numpy().tolist()\n",
    "    return ret\n",
    "\n",
    "def setParameters(ann, meanVec):\n",
    "    ind = 0\n",
    "    for e in ann.parameters():\n",
    "        shape = e.size()\n",
    "        nParams = np.prod(shape)\n",
    "        e.data = torch.Tensor(np.array(meanVec[ind:ind + nParams]).reshape(*shape))\n",
    "        ind += nParams\n",
    "        \n",
    "\n",
    "@ray.remote\n",
    "class Runner:\n",
    "    def __init__(self, env_name, actor_id):\n",
    "        self.env = make(env_name)\n",
    "        self.id = actor_id\n",
    "        self.policy = Actor()\n",
    "        self.critic = Critic()\n",
    "        self.i = 0\n",
    "        self.stats_len = 100\n",
    "        self.target_len = 800\n",
    "        self.episode_len = 200\n",
    "        self.update_len = 10\n",
    "        self.entropy = 0.01\n",
    "        self.gamma = 0.9\n",
    "        self.total_reward = 0\n",
    "        self.target = deepcopy(self.critic)\n",
    "        \n",
    "    \n",
    "    def gather_rollout(self):\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        policies = []\n",
    "        actions = []\n",
    "        values = []\n",
    "        returns = []\n",
    "        entropy = []\n",
    "        logProb = []\n",
    "        targets = []\n",
    "        if self.i % self.episode_len == 0:\n",
    "            self.state = self.env.reset()\n",
    "        \n",
    "        for epoch in range(1):\n",
    "            rewards = []\n",
    "            for i in range(self.update_len):\n",
    "                mu, sigma = self.policy(torch.from_numpy(self.state).float())\n",
    "                val = self.critic(torch.from_numpy(self.state).float())\n",
    "                target = self.target(torch.from_numpy(self.state).float())\n",
    "                pi = Normal(mu, sigma)\n",
    "                action = pi.sample().detach()\n",
    "                self.next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                reward = (reward + 8.1) / 8.1\n",
    "                self.i += 1\n",
    "                rewards.append(reward)\n",
    "                actions.append(action)\n",
    "                values.append(val)\n",
    "                targets.append(target)\n",
    "                logProb.append(pi.log_prob(action))\n",
    "                entropy.append(pi.entropy())\n",
    "                self.state = self.next_state\n",
    "            self.total_reward += total_reward\n",
    "            next_target = torch.zeros(1) if self.i % self.episode_len == 0 else self.target(torch.from_numpy(self.next_state).float())\n",
    "            returns = torch.tensor(rewards).view(-1, 1) + self.gamma * torch.cat((torch.cat(targets[1:]), next_target)).view(-1, 1).detach()\n",
    "\n",
    "        return {'logProb': torch.cat(logProb).view(-1, 1),\n",
    "                'entropy': torch.cat(entropy).view(-1, 1),\n",
    "                'rets': returns,\n",
    "                'actions':  torch.cat(actions).view(-1, 1),\n",
    "                'vals':  torch.cat(values).view(-1, 1)}, self.total_reward\n",
    "\n",
    "    def backward(self, rollout):\n",
    "        adv = advantage(rollout['rets'] - rollout['vals'])\n",
    "        polLoss = policy_loss(rollout['logProb'], adv)\n",
    "        entLoss = -rollout['entropy'].mean() * self.entropy\n",
    "        valLoss = value_loss(rollout['vals'], rollout['rets'])\n",
    "        totLoss = valLoss + entLoss + polLoss\n",
    "        totLoss.backward()\n",
    "        return getGrads(self.policy), getGrads(self.critic), float(valLoss), float(entLoss), float(polLoss) \n",
    "\n",
    "    def compute_gradient(self, params, critic):\n",
    "        setParameters(self.policy, params[0])\n",
    "        zeroGrads(self.policy)\n",
    "        setParameters(self.critic, critic[0])\n",
    "        zeroGrads(self.critic)\n",
    "        if self.i % self.target_len == 0:\n",
    "            self.target = deepcopy(self.critic)\n",
    "        rollout, total_reward = self.gather_rollout()\n",
    "        grads, gradsCritic, valLoss, entLoss, polLoss = self.backward(rollout)\n",
    "        info = {\"id\": self.id,\n",
    "                'valLoss': valLoss,\n",
    "                'entLoss': entLoss,\n",
    "                'polLoss': polLoss,\n",
    "                'action': rollout['actions'].mean()}\n",
    "        if self.i % self.stats_len == 0:\n",
    "            info[\"totalReward\"] =  self.total_reward\n",
    "            self.total_reward = 0\n",
    "        return {0: grads}, {0: gradsCritic} , info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-B2vMRnuRgu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "\n",
    "\n",
    "def getParameters(ann):\n",
    "    ret = []\n",
    "    for name, e in ann.named_parameters():\n",
    "        ret += e.data.view(-1).numpy().tolist()\n",
    "    return ret\n",
    "\n",
    "  \n",
    "class ManualAdam(optim.Adam):\n",
    "    def step(self, grads):\n",
    "        grads = Variable(torch.Tensor(np.array(grads)))\n",
    "        self.param_groups[0]['params'][0].grad = grads\n",
    "        super().step()\n",
    "        \n",
    "def stepOpt(opt, gradDict, params):  \n",
    "        grads = defaultdict(list)\n",
    "        for worker, grad in gradDict.items():\n",
    "                grads[worker].append(grad)\n",
    "\n",
    "        gradAry = torch.zeros_like(params)\n",
    "\n",
    "        for worker, gradList in grads.items():\n",
    "            grad = np.array(gradList)\n",
    "            grad = np.mean(grad, 0)\n",
    "            grad = np.clip(grad, -5, 5)\n",
    "            gradAry[worker] = torch.Tensor(grad)\n",
    "\n",
    "        opt.step(gradAry)\n",
    "        \n",
    "        \n",
    "def train(num_workers, env_name=\"Pendulum-v0\"):\n",
    "    policy = Actor()\n",
    "    critic = Critic()\n",
    "    model = [getParameters(policy)]\n",
    "    modelCritic = [getParameters(critic)]\n",
    "    params = Parameter(torch.Tensor(np.array(model)))\n",
    "    paramsCritic = Parameter(torch.Tensor(np.array(modelCritic)))\n",
    "    opt = ManualAdam([params], lr=0.0001)\n",
    "    optCritic = ManualAdam([paramsCritic], lr=0.001)\n",
    "    wandb.init()\n",
    "    ray.init()\n",
    "\n",
    "    agents = [Runner.remote(env_name, i) for i in range(num_workers)]\n",
    "    gradient_list = [agent.compute_gradient.remote(params.detach().numpy(), paramsCritic.detach().numpy()) for agent in agents]\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        done_id, gradient_list = ray.wait(gradient_list)\n",
    "\n",
    "        grads, gradsCritic, info = ray.get(done_id)[0]\n",
    "        wandb.log(info)\n",
    "        i += 1\n",
    "        stepOpt(opt, grads, params)\n",
    "        opt.zero_grad()\n",
    "        stepOpt(optCritic, gradsCritic, paramsCritic)\n",
    "        optCritic.zero_grad()\n",
    "        if 'totalReward' in info.keys() and info['totalReward'] > -10:\n",
    "            setParameters(policy, params.detach().numpy()[0])\n",
    "            policy.save()\n",
    "        parameters = params.detach().numpy()\n",
    "        parCritic = paramsCritic.detach().numpy()\n",
    "        gradient_list.extend([agents[info[\"id\"]].compute_gradient.remote(parameters, parCritic)])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJrcU9RyOymq"
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXdxkZWhOym0"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def ortho_init(module, nonlinearity=None, weight_scale=1.0, constant_bias=0.0):\n",
    "    r\"\"\"Applies orthogonal initialization for the parameters of a given module.\n",
    "    \n",
    "    Args:\n",
    "        module (nn.Module): A module to apply orthogonal initialization over its parameters. \n",
    "        nonlinearity (str, optional): Nonlinearity followed by forward pass of the module. When nonlinearity\n",
    "            is not ``None``, the gain will be calculated and :attr:`weight_scale` will be ignored. \n",
    "            Default: ``None``\n",
    "        weight_scale (float, optional): Scaling factor to initialize the weight. Ignored when\n",
    "            :attr:`nonlinearity` is not ``None``. Default: 1.0\n",
    "        constant_bias (float, optional): Constant value to initialize the bias. Default: 0.0\n",
    "        \n",
    "    .. note::\n",
    "    \n",
    "        Currently, the only supported :attr:`module` are elementary neural network layers, e.g.\n",
    "        nn.Linear, nn.Conv2d, nn.LSTM. The submodules are not supported.\n",
    "    \n",
    "    Example::\n",
    "    \n",
    "        >>> a = nn.Linear(2, 3)\n",
    "        >>> ortho_init(a)\n",
    "    \n",
    "    \"\"\"\n",
    "    if nonlinearity is not None:\n",
    "        gain = nn.init.calculate_gain(nonlinearity)\n",
    "    else:\n",
    "        gain = weight_scale\n",
    "        \n",
    "    if isinstance(module, (nn.RNNBase, nn.RNNCellBase)):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_' in name:\n",
    "                nn.init.orthogonal_(param, gain=gain)\n",
    "            elif 'bias_' in name:\n",
    "                nn.init.constant_(param, constant_bias)\n",
    "    else:  # other modules with single .weight and .bias\n",
    "        nn.init.orthogonal_(module.weight, gain=gain)\n",
    "        nn.init.constant_(module.bias, constant_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "with open(\"agent.pkl\", \"rb\") as f:\n",
    "        weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zG0jUbcvOym3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6bc76a4ecf13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m  \u001b[0;31m#   pi = Normal(mu, sigma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "from gym import make\n",
    "from torch.nn import functional as F\n",
    "\n",
    "env = make(\"Pendulum-v0\")\n",
    "\n",
    "for epoch in range(100):\n",
    " state = env.reset()\n",
    " for i in range(75):\n",
    "    \n",
    "    mu, sigma = weights(torch.from_numpy(state).float())\n",
    "    action = mu.detach()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
