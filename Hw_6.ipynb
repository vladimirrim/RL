{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hw_6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAWNq2Ox3N6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "BETA =  150\n",
        "\n",
        "\n",
        "class RNDModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.target = nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512)\n",
        "        )\n",
        "\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(input_size, 512)\n",
        "        )\n",
        "\n",
        "        for p in self.modules():\n",
        "            if isinstance(p, nn.Conv2d):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "            if isinstance(p, nn.Linear):\n",
        "                init.orthogonal_(p.weight, np.sqrt(2))\n",
        "                p.bias.data.zero_()\n",
        "\n",
        "        for param in self.target.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, next_obs):\n",
        "        target_feature = self.target(next_obs)\n",
        "        predict_feature = self.predictor(next_obs)\n",
        "\n",
        "        return predict_feature, target_feature\n",
        "\n",
        "\n",
        "class Exploration:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.distillery = RNDModel(state_dim)\n",
        "        self.lr = 0.0001\n",
        "        self.opt = Adam(self.distillery.parameters(), lr=self.lr)\n",
        "\n",
        "    def get_exploration_reward(self, states, actions, next_states):\n",
        "        pred, target = self.distillery(next_states)\n",
        "        return ((pred - target) ** 2).sum(1).view(-1, 1)\n",
        "\n",
        "    def update(self, state):\n",
        "        pred, target = self.distillery(state)\n",
        "        loss = ((pred - target) ** 2).sum()\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "     #   wandb.log({\"explo\": loss})\n",
        "        self.opt.step()\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim  # dimensionalite of state space\n",
        "        self.action_dim = action_dim  # count of available actions\n",
        "        self.exploration = Exploration(state_dim, action_dim)\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.path = []\n",
        "        self.model = self.build_model()\n",
        "        self.target_model = copy.deepcopy(self.model)\n",
        "        self.lr = 0.001\n",
        "        self.eps = 0.9\n",
        "        self.gamma = 0.9\n",
        "        self.opt = Adam(self.model.parameters(), lr=self.lr, weight_decay=0.00001)\n",
        "        self.go = False\n",
        "        self.pos = 0\n",
        "        self.pos_best = 0\n",
        "        self.step = 0\n",
        "        self.cur = 0\n",
        "        self.prev_ac = 0\n",
        "        self.best_path = []\n",
        "        self.tmp_path = []\n",
        "        self.best_surprise = 0\n",
        "\n",
        "    def build_model(self):\n",
        "        model = nn.Sequential(nn.Linear(self.state_dim, 64),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(64, 64),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(64, self.action_dim))\n",
        "        return model\n",
        "\n",
        "    def act_model(self, state):\n",
        "            num = np.random.rand()\n",
        "            if (num < self.eps):\n",
        "                if np.random.rand() < 0:\n",
        "                    return self.prev_ac\n",
        "                else:\n",
        "                    return np.random.randint(0, self.action_dim)\n",
        "            else:\n",
        "                return torch.argmax(self.model(torch.from_numpy(state).float()).detach()).numpy()\n",
        "    \n",
        "    def act(self, state):\n",
        "        if self.go:\n",
        "            if self.pos >= len(self.path):\n",
        "                self.go = False\n",
        "                action = self.act_model(state)\n",
        "            else:\n",
        "                action = self.path[self.pos]\n",
        "                self.pos += 1\n",
        "        else:\n",
        "            if self.pos_best >= len(self.best_path):\n",
        "                action = self.act_model(state)\n",
        "            else:\n",
        "                action = self.best_path[self.pos_best]\n",
        "                self.pos_best += 1\n",
        "        return action\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.step += 1\n",
        "        self.eps *= 0.99\n",
        "        self.best_surprise *= 0.999\n",
        "        state, action, next_state, reward, done = transition\n",
        "        self.prev_ac = action\n",
        "        if not self.go:\n",
        "            self.path.append(action)\n",
        "            if done:\n",
        "              #  print(len(self.tmp_path))\n",
        "                self.best_path = copy.deepcopy(self.tmp_path)\n",
        "                self.pos = 0\n",
        "                self.best_surprise = 0\n",
        "                self.pos_best = 0\n",
        "                if reward <= 0:\n",
        "                  self.path.clear()\n",
        "            if done and reward > 0:\n",
        "                self.go = True\n",
        "        else:\n",
        "            if done:\n",
        "            #    print(self.tmp_path)\n",
        "                self.best_path = copy.deepcopy(self.tmp_path)\n",
        "                self.pos = 0\n",
        "                self.best_surprise = 0\n",
        "                self.pos_best = 0\n",
        "        ext = BETA * self.exploration.get_exploration_reward(state, action, torch.from_numpy(next_state).float().unsqueeze(0)).detach().clamp(max=1.0).item()\n",
        "        self.exploration.update(torch.from_numpy(next_state).float().unsqueeze(0))\n",
        "        if self.best_surprise < ext and not done and len(self.path) < 120:\n",
        "          self.best_surprise = ext\n",
        "          self.tmp_path = copy.deepcopy(self.path)\n",
        "        reward += self.gamma * ext - self.cur\n",
        "        self.cur = int(done) * ext\n",
        "        self.memory.append((state, action, next_state, reward, done))\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        state, action, next_state, reward, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action = torch.tensor(np.array(action))\n",
        "     #   wandb.log({\"action\": action.float().mean()})\n",
        "        dones = torch.tensor(done).float()\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "        reward = torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "        targets = reward + self.gamma * self.target_model(next_state).detach().max(1)[0] * (1 - dones)\n",
        "        loss =  F.smooth_l1_loss(self.model(state).gather(1, action.view(-1, 1).long()).squeeze(), targets.squeeze())\n",
        "        #loss = loss.mean()\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "       # wandb.log({\"dqn\": loss})\n",
        "        self.opt.step()\n",
        "        if self.step % 400 == 0:\n",
        "          self.target_model = copy.deepcopy(self.model)\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrJaniVdsFgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym import make\n",
        "\n",
        "env = make(\"MountainCar-v0\")\n",
        "algo = Agent(state_dim=2, action_dim=3)\n",
        "episodes = 150\n",
        "visit_count = 0\n",
        "\n",
        "#wandb.init()\n",
        "\n",
        "for i in range(episodes):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = algo.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = next_state\n",
        "        steps += 1\n",
        "        algo.update((state, action, next_state, reward, done))\n",
        "        state = next_state\n",
        "    print(i)\n",
        "    if steps < 200:\n",
        "        visit_count += 1\n",
        "        print(\"Visited target state at episode\", i)\n",
        "print()\n",
        "print(\"Total visit count:\", visit_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU01BqevxUvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ojcIe85-mN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}